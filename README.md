# Cosmos Ã— LeIsaac: Video-to-Action Data Generation Pipeline

This tutorial extends **LeIsaac** by integrating **Cosmos-Predict2.5** and **GR00T-Dreams IDM** into a LeIsaac-native data generation loop. **LeIsaac** is used to collect teleoperated demonstrations (HDF5) and convert them into LeRobot datasets. **Cosmos-Predict 2.5** is post-trained on these videos to synthesize additional rollout videos at scale, and **IDM** is fine-tuned on the same dataset to infer robot actions from the generated videos. Together, this produces **a scalable pipeline for constructing synthetic, complete LeRobot datasets** , which can be replayed and evaluated directly in LeIsaac.

---

## Overview

1. Use **LeIsaac** to collect an **HDF5 dataset** and convert it into a **LeRobot dataset** 
2. Post-training **Cosmos-Predict2.5**, then run inference to **generate synthetic videos** 
3. Fine-tune **IDM**, then run inference to **generate synthetic LeRobot trajectories(parquet)**  
4. Use **LeIsaac** to **replay and evaluate** using the HDF5 dataset + inferred action trajectories 
   
---

## 1.Data Collection with LeIsaac

### What you get from this step

- A real demonstration dataset in **HDF5** format (recorded in Isaac Sim through LeIsaac teleoperation)
- A converted dataset in **LeRobot** format (videos + metadata + parquet)

---

### 1.1 Collect the HDF5 Dataset via LeIsaac Teleoperation

First, collect demonstrations using **LeIsaac** teleoperation.

- Reference: **[Teleoperation | LeIsaac Document](https://lightwheelai.github.io/leisaac/docs/getting_started/teleoperation)**
- Output: an HDF5 file (for example, `dataset.hdf5`)

The HDF5 dataset serves as the primary source of truth for replay and evaluation.  
It is also the source used to construct the initial **LeRobot dataset** for both Cosmos and IDM.

---

### 1.2 Convert HDF5 to a LeRobot Dataset

Convert the recorded HDF5 dataset into **LeRobot format**.

- Reference: **[Data Convention | LeIsaac Document](https://lightwheelai.github.io/leisaac/docs/getting_started/policy_support#1-data-convention)**
- Output: a LeRobot dataset used for post-training  **Cosmos-Predict2.5** and fine-tuning **IDM**

> **ðŸ’¡Important (Video Encoding)**
>  
> Ensure that all output videos are encoded using **H.264 (h264)**.  
> Avoid **AV1** encoding, as Cosmos and IDM may fail during decoding or processing.  

You can directly modify the `scripts/convert/isaaclab2lerobot.py` to ensure correct video encoding:
```python
#from
"video.codec": "av1"
#to:
"video.codec": "h264"
```
---

## 2.Video Generation with Cosmos-predict2.5

### What you get from this step

- A **Cosmos-Predict2.5 checkpoint** post-trained on your task-specific LeRobot videos  
- A set of **synthetic rollout videos** generated by Cosmos  

In this pipeline, **Cosmos-Predict 2.5** is used purely as a **video generator**.  
It learns the visual rollout distribution from LeRobot videos collected via **LeIsaac**, and generates new rollout videos conditioned on:
- a text prompt (task description),
- and the first few frames of an example videos.

These generated videos will later be converted into executable robot actions using **IDM**.

---

### 2.1 Install Cosmos-Predict2.5

Set up the Cosmos-Predict2.5 environment by following the official installation guide:**[Set up the Cosmos-Predict2.5](https://github.com/nvidia-cosmos/cosmos-predict2.5/blob/main/docs/setup.md)**

---

### 2.2 Prepare the Cosmos-Predict2.5 Post-Training Dataset (Video + Prompt)

Dataset folder format should be:
```
cosmos-predict2.5/datasets/benchmark_train<task_name>/
â”œâ”€â”€ metas/
â”‚   â”œâ”€â”€ *.txt
â”œâ”€â”€ videos/
â”‚   â”œâ”€â”€ *.mp4
```
How to construct it from your LeRobot dataset:

1.Copy MP4 videos from your LeRobot dataset from `your_lerobot_dataset/videos/`  to `cosmos-predict2.5/datasets/benchmark_train/<task_name>/videos/`

2.Rename the copied videos as a clean numeric sequence:`1.mp4`, `2.mp4`, `3.mp4`, ...

3.Create the same number of prompt files under `cosmos-predict2.5/datasets/benchmark_train/<task_name>/metas/`

4.Fill each prompt file using the task text in `your_lerobot_dataset/meta/tasks.jsonl`.For example, if one line in tasks.jsonl is `{"task_index": 0, "task": "Lift the red cube up."}`,then 1.txt should contain `Lift_the_red_cube_up`.In many single-task cases, you will write the same prompt into all *.txt files, but the format supports per-video prompts if needed.

---

### 2.3 Post-training Cosmos-Predict2.5

Post-training Cosmos-Predict2.5 on the prepared dataset by following **[the official post-training instructions](https://github.com/nvidia-cosmos/cosmos-predict2.5/blob/main/docs/post-training_video2world_gr00t.md#21-post-training-cosmos-predict25-2b-model)**:
This step produces a Cosmos checkpoint specialized for your robot embodiment and task distribution.

---

### 2.4 Run Inference to Generate Videos

After post-training, the Cosmos-Predict2.5 model checkpoints are typically saved in **Distributed Checkpoint (DCP)** format.Before running inference, these checkpoints need to be converted into a **consolidated PyTorch format** that can be loaded by the inference scripts.

**Follow the [official instructions ](https://github.com/nvidia-cosmos/cosmos-predict2.5/blob/main/docs/post-training_video2world_gr00t.md#31-converting-dcp-checkpoint-to-consolidated-pytorch-format) to convert DCP checkpoints**.

Once the checkpoint has been converted, run video generation inference following the [official instructions ](https://github.com/nvidia-cosmos/cosmos-predict2.5/blob/main/docs/post-training_video2world_gr00t.md#32-running-inference).  


#### ðŸ”¬**Batch Inference (Used for fine-tuning IDM)**

For large-scale video generation, batch inference is supported.  
You can generate a batch inference configuration file (for example, `batch_inference_config.jsonl`) using:

```bash
python scripts/generate_batch_config.py --use-prompt
```

---
## 3.Action Inference with IDM


---
## 4.Replay and Evaluation in LeIsaac

